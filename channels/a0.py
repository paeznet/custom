# -*- coding: iso-8859-1 -*-
#------------------------------------------------------------
#------------------------------------------------------------
Por regla general, cuando sepas que en Py3 te puede venir como Bytes, debes usar esto para convertirlo a Str normal:

if PY3 and isinstance(url, bytes):
    url = url.decode('utf-8')

base64.b64encode(VARIABLE.encode('utf8')).decode('utf8')
base64.b64decode(VARIABLE).decode('utf-8')

En general, cuando veas un VARAIBLE.encode('utf8') en el código, tendrás que poner:
if not PY3:
    data = unicode(data, "utf-8", errors="replace").encode("utf-8")



import sys
PY3 = False
if sys.version_info[0] >= 3: PY3 = True; unicode = str; unichr = chr; long = int

if PY3:
    import urllib.parse as urlparse                             # Es muy lento en PY2.  En PY3 es nativo
else:
    import urlparse                                             # Usamos el nativo de PY2 que es más rápido

import re



import urlparse,urllib2,urllib,re
import os, sys

from platformcode import config, logger
from core import scrapertools
from core.item import Item
from core import servertools
from core import httptools
from core import tmdb
from core import jsontools as json
## italiafilm

import ssl
logger.error(ssl.OPENSSL_VERSION_INFO) #version de SSL
logger.error(sys.version)   #version de phyton

                        #          SALYNPM,  EstrenosCineSaa   Peliserieslive(json) mobidy peliculasFlix  pelisflix
                        #                                     pelisencastellano



        "pattern": "(http://www.(?:nuvid|drtuber|iceporn|viptube|vivatube|tubeon|hd21|yeptube|winporn).com/embed/[0-9]+)",


                    # THUMBNAIL
        https://i.postimg.cc/QN24c489/pornxbit.png








https://hdzog.com/latest-updates/
http://192.168.0.19:48884/getSourceByPageFinished?url=aHR0cHM6Ly9oZHpvZy5jb20vbGF0ZXN0LXVwZGF0ZXMv&time=5000



http://192.168.0.19:48884/getSourceByPageFinished?url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PVNtVkJTbE5VU0ZBMmVWVnJMMHQzYVRCQk5UUndVWG9yVHk5WU1rWlZObWwyVEhSVmMyUkxSRlpoWXpoUE5YQk5PV0Y2U25sRU1ERlJTMFpTVWpSWlpRJTNEJTNEI2lzcz1NVGcxTGpFd09DNHlORGt1TVRneQ==&debug=true&cache=false&time=20000&jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICB9IGNhdGNoIChlKSB7CiAgICAgICAgY29uc29sZS5lcnJvcignIyNFcnJvciBnZXR0aW5nIGlmcmFtZSBmb2N1cycsIGUpOwogICAgfTsKfSkpKCk7
                url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PVNtVkJTbE5VU0ZBMmVWVnJMMHQzYVRCQk5UUndVWG9yVHk5WU1rWlZObWwyVEhSVmMyUkxSRlpoWXpoUE5YQk5PV0Y2U25sRU1ERlJTMFpTVWpSWlpRJTNEJTNEI2lzcz1NVGcxTGpFd09DNHlORGt1TVRneQ==
                        https://waaw.to/watch_video.php?v=SmVBSlNUSFA2eVVrL0t3aTBBNTRwUXorTy9YMkZVNml2THRVc2RLRFZhYzhPNXBNOWF6SnlEMDFRS0ZSUjRZZQ%3D%3D#iss=MTg1LjEwOC4yNDkuMTgy
                &debug=true&cache=false&time=20000
                &jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICB9IGNhdGNoIChlKSB7CiAgICAgICAgY29uc29sZS5lcnJvcignIyNFcnJvciBnZXR0aW5nIGlmcmFtZSBmb2N1cycsIGUpOwogICAgfTsKfSkpKCk7
                                ((() => {
                                    try {
                                        document.querySelectorAll('iframe')[0].focus();
                                    } catch (e) {
                                        console.error('##Error getting iframe focus', e);
                                    };
                                }))();


http://192.168.0.101:48884/getSourceByPageFinished?url=aHR0cHM6Ly93d3cucG9ybmRpc2guY29tL2JpZ3RpdHNyb3VuZGFzc2VzLWFubmFiZWwtcmVkZC1iaWctdGl0dHktam95LXJpZGUv&time=5000&debug=true&cache=false

https://waaw.to/watch_video.php?v=RzRpYTJxRy9WakR3TjJ3NGI2TStzcStTd2ZhRFZyQnBJSEc3a0szSlBUeUdqWlNFWGNZYWNpUXlwcEJBZ3MzTQ%3D%3D
http://192.168.0.101:48884/getSourceByPageFinished?debug=true&cache=false&time=20000&jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICAgICBmb3IgKHZhciBpID0gMTsgaSA8PSAxMDsgaSsrKSB7CiAgICAgICAgICAvL2FsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9EUEFEX0NFTlRFUicpOwogICAgICAgICAgaWYgKGkgPT0gMTApIHsKCSAgICAgICAgYWxmYUFzc2lzdGFudEFuZHJvaWRQSS5zZW5kS2V5KCdLRVlDT0RFX0VOVEVSJyk7CiAgICAgICAgICB9CgkgIGFsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9UQUInKTsKICAgICAgIH0KICAgIH0gY2F0Y2ggKGUpIHsKICAgICAgICBjb25zb2xlLmVycm9yKCcjI0Vycm9yIGdldHRpbmcgaWZyYW1lIGZvY3VzJywgZSk7CiAgICB9Owp9KSkoKTsK&extraPostDelay=8000&url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PVJ6UnBZVEp4Unk5V2FrUjNUakozTkdJMlRTdHpjU3RUZDJaaFJGWnlRbkJKU0VjM2Ewc3pTbEJVZVVkcVdsTkZXR05aWVdOcFVYbHdjRUpCWjNNelRRJTNEJTNE

http://192.168.0.101:48884/getSourceByPageFinished?debug=true&cache=false&time=20000
    &jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICAgICBmb3IgKHZhciBpID0gMTsgaSA8PSAxMDsgaSsrKSB7CiAgICAgICAgICAvL2FsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9EUEFEX0NFTlRFUicpOwogICAgICAgICAgaWYgKGkgPT0gMTApIHsKCSAgICAgICAgYWxmYUFzc2lzdGFudEFuZHJvaWRQSS5zZW5kS2V5KCdLRVlDT0RFX0VOVEVSJyk7CiAgICAgICAgICB9CgkgIGFsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9UQUInKTsKICAgICAgIH0KICAgIH0gY2F0Y2ggKGUpIHsKICAgICAgICBjb25zb2xlLmVycm9yKCcjI0Vycm9yIGdldHRpbmcgaWZyYW1lIGZvY3VzJywgZSk7CiAgICB9Owp9KSkoKTsK
    &url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PVJ6UnBZVEp4Unk5V2FrUjNUakozTkdJMlRTdHpjU3RUZDJaaFJGWnlRbkJKU0VjM2Ewc3pTbEJVZVVkcVdsTkZXR05aWVdOcFVYbHdjRUpCWjNNelRRJTNEJTNE

https://waaw.to/watch_video.php?v=aDR6cDh0c3FQak9BYUlkamFVb1p5RFFlL21NSTBlYVpJZVQrWStLQjBycSt2N2JMV1l0THMwdHNKNzJsMVZGdQ%3D%3D#iss=MTg1LjEwOC4yNDkuMTgy
http://192.168.0.101:48884/getSourceByPageFinished?cache=false&time=20000&jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICAgICBmb3IgKHZhciBpID0gMTsgaSA8PSAxMDsgaSsrKSB7CiAgICAgICAgICAvL2FsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9EUEFEX0NFTlRFUicpOwogICAgICAgICAgaWYgKGkgPT0gMTApIHsKCSAgICAgICAgYWxmYUFzc2lzdGFudEFuZHJvaWRQSS5zZW5kS2V5KCdLRVlDT0RFX0VOVEVSJyk7CiAgICAgICAgICB9CgkgIGFsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9UQUInKTsKICAgICAgIH0KICAgIH0gY2F0Y2ggKGUpIHsKICAgICAgICBjb25zb2xlLmVycm9yKCcjI0Vycm9yIGdldHRpbmcgaWZyYW1lIGZvY3VzJywgZSk7CiAgICB9Owp9KSkoKTsK&extraPostDelay=8000&url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PWFEUjZjRGgwYzNGUWFrOUJZVWxrYW1GVmIxcDVSRkZsTDIxTlNUQmxZVnBKWlZRcldTdExRakJ5Y1N0Mk4ySk1WMWwwVEhNd2RITktOekpzTVZaR2RRJTNEJTNEI2lzcz1NVGcxTGpFd09DNHlORGt1TVRneQ==
        jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICAgICBmb3IgKHZhciBpID0gMTsgaSA8PSAxMDsgaSsrKSB7CiAgICAgICAgICAvL2FsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9EUEFEX0NFTlRFUicpOwogICAgICAgICAgaWYgKGkgPT0gMTApIHsKCSAgICAgICAgYWxmYUFzc2lzdGFudEFuZHJvaWRQSS5zZW5kS2V5KCdLRVlDT0RFX0VOVEVSJyk7CiAgICAgICAgICB9CgkgIGFsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9UQUInKTsKICAgICAgIH0KICAgIH0gY2F0Y2ggKGUpIHsKICAgICAgICBjb25zb2xlLmVycm9yKCcjI0Vycm9yIGdldHRpbmcgaWZyYW1lIGZvY3VzJywgZSk7CiAgICB9Owp9KSkoKTsK&extraPostDelay=8000
        &url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PWFEUjZjRGgwYzNGUWFrOUJZVWxrYW1GVmIxcDVSRkZsTDIxTlNUQmxZVnBKWlZRcldTdExRakJ5Y1N0Mk4ySk1WMWwwVEhNd2RITktOekpzTVZaR2RRJTNEJTNEI2lzcz1NVGcxTGpFd09DNHlORGt1TVRneQ==
                        
 https://pepecine.com/secure/search/thor?type=&limit=30
 http://192.168.0.101:48884/getSourceByPageFinished?url=aHR0cHM6Ly9wZXBlY2luZS5jb20vc2VjdXJlL3NlYXJjaC90aG9yP3R5cGU9JmxpbWl0PTMw&time=15000&getCookies=true&cache=false&debug=true&removeAllCookies=true&clearWebCache=true&jsCode=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&extraPostDelay=10000&userAgent=TW96aWxsYS81LjAgKExpbnV4OyBBbmRyb2lkIDQuMC40OyBHYWxheHkgTmV4dXMgQnVpbGQvSU1NNzZCKSBBcHBsZVdlYktpdC81MzUuMTkgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvMTguMC4xMDI1LjEzMyBNb2JpbGUgU2FmYXJpLzUzNS4xOQ==

http://192.168.0.101:48884/getSourceByPageFinished?url=aHR0cHM6Ly9wZXBlY2luZS5jb20vc2VjdXJlL3NlYXJjaC90aG9yP3R5cGU9JmxpbWl0PTMw&time=15000&getCookies=true&cache=false&debug=true&removeAllCookies=true&clearWebCache=true&jsCode=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&extraPostDelay=10000&userAgent=TW96aWxsYS81LjAgKExpbnV4OyBBbmRyb2lkIDQuMC40OyBHYWxheHkgTmV4dXMgQnVpbGQvSU1NNzZCKSBBcHBsZVdlYktpdC81MzUuMTkgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvMTguMC4xMDI1LjEzMyBNb2JpbGUgU2FmYXJpLzUzNS4xOQ==


https://czechvideo.org/20973-teenslovemoney-ava-taylor-windy-city-snatch.html
aHR0cHM6Ly9jemVjaHZpZGVvLm9yZy8yMDk3My10ZWVuc2xvdmVtb25leS1hdmEtdGF5bG9yLXdpbmR5LWNpdHktc25hdGNoLmh0bWw=
 http://192.168.0.101:48884/getSourceByPageFinished?debug=true&cache=false&time=20000&jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICAgICBmb3IgKHZhciBpID0gMTsgaSA8PSAxMDsgaSsrKSB7CiAgICAgICAgICAvL2FsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9EUEFEX0NFTlRFUicpOwogICAgICAgICAgaWYgKGkgPT0gMTApIHsKCSAgICAgICAgYWxmYUFzc2lzdGFudEFuZHJvaWRQSS5zZW5kS2V5KCdLRVlDT0RFX0VOVEVSJyk7CiAgICAgICAgICB9CgkgIGFsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9UQUInKTsKICAgICAgIH0KICAgIH0gY2F0Y2ggKGUpIHsKICAgICAgICBjb25zb2xlLmVycm9yKCcjI0Vycm9yIGdldHRpbmcgaWZyYW1lIGZvY3VzJywgZSk7CiAgICB9Owp9KSkoKTsK&extraPostDelay=8000&url=aHR0cHM6Ly9jemVjaHZpZGVvLm9yZy8yMDk3My10ZWVuc2xvdmVtb25leS1hdmEtdGF5bG9yLXdpbmR5LWNpdHktc25hdGNoLmh0bWw=

 
 #iss=MTkzLjExMS41Mi4xMzA=
 https://waaw.to/watch_video.php?v=NHlrYlpuTldTZ1RrUGJtK1lrdGlISEhrZVlNcElNMEppK2tDUXFGTTJyYWV5Y2JES2J4eWlkanFMckVuenpiSQ%3D%3D#iss=MTkzLjExMS41Mi4xMzA=
https://czxxx.org/player/embed_player.php?vid=ghCCizxamK8S
https://waaw.to/watch_video.php?v=ghCCizxamK8S
http://192.168.0.101:48884/getSourceByPageFinished?debug=true&cache=false&time=20000&jsCode=KCgoKSA9PiB7CiAgICB0cnkgewogICAgICAgIGRvY3VtZW50LnF1ZXJ5U2VsZWN0b3JBbGwoJ2lmcmFtZScpWzBdLmZvY3VzKCk7CiAgICAgICBmb3IgKHZhciBpID0gMTsgaSA8PSAxMDsgaSsrKSB7CiAgICAgICAgICAvL2FsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9EUEFEX0NFTlRFUicpOwogICAgICAgICAgaWYgKGkgPT0gMTApIHsKCSAgICAgICAgYWxmYUFzc2lzdGFudEFuZHJvaWRQSS5zZW5kS2V5KCdLRVlDT0RFX0VOVEVSJyk7CiAgICAgICAgICB9CgkgIGFsZmFBc3Npc3RhbnRBbmRyb2lkUEkuc2VuZEtleSgnS0VZQ09ERV9UQUInKTsKICAgICAgIH0KICAgIH0gY2F0Y2ggKGUpIHsKICAgICAgICBjb25zb2xlLmVycm9yKCcjI0Vycm9yIGdldHRpbmcgaWZyYW1lIGZvY3VzJywgZSk7CiAgICB9Owp9KSkoKTsK&extraPostDelay=8000&url=aHR0cHM6Ly93YWF3LnRvL3dhdGNoX3ZpZGVvLnBocD92PU5IbHJZbHB1VGxkVFoxUnJVR0p0SzFscmRHbElTRWhyWlZsTmNFbE5NRXBwSzJ0RFVYRkdUVEp5WVdWNVkySkVTMko0ZVdsa2FuRk1ja1Z1ZW5waVNRJTNEJTNEI2lzcz1NVGt6TGpFeE1TNDFNaTR4TXpBPQo=


###############     TIMEOUT
        timeout = config.get_setting('timeout_downloadpage', channel)
        data = httptools.downloadpage(item.url, timeout=timeout).data

        ################         CUNMINATION LOG    utils.kodilog(url)        ############### )


##############JSON
    {
      "id": "timeout_downloadpage",
      "type": "list",
      "label": "Timeout (segs.) en descarga de páginas o verificación de servidores",
      "default": 5,
      "enabled": true,
      "visible": true,
      "lvalues": [
        "None",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "10",
        "11",
        "12",
        "13",
        "14",
        "15"
      ]
    },



####################   animefenix   comamosramen

def get_source(url, headers={}, post={}, soup=False, unescape=False):
    logger.info()

    if post:
        if soup:
            data = httptools.downloadpage(url, post=post, headers=headers, soup=True, canonical=canonical).soup
        else:
            data = httptools.downloadpage(url, post=post, headers=headers, canonical=canonical).data
    else:
        if soup:
            data = httptools.downloadpage(url, soup=True, headers=headers, canonical=canonical).soup
        else:
            data = httptools.downloadpage(url, headers=headers, canonical=canonical).data

        if unescape:
            data = scrapertools.unescape(data)

    return data


def get_source(url, json=False, soup=False, multipart_post=None, timeout=30, add_host=True, **opt):
    logger.info()

    opt['canonical'] = canonical
    data = httptools.downloadpage(url, soup=soup, files=multipart_post, add_host=add_host, timeout=timeout, **opt)

    # Verificamos que tenemos una sesión válida, sino, no tiene caso devolver nada
    if "Iniciar sesión" in data.data:
        # Si no tenemos sesión válida, mejor cerramos definitivamente la sesión
        global account
        if account: logout({})
        platformtools.dialog_notification("No se ha inciado sesión", "Inicia sesión en el canal {} para poder usarlo".format(__channel__))
        return None

    if json:
        data = data.json
    elif soup:
        data = data.soup
    else:
        data = data.data

    return data

soup = get_source(item.url, soup=True)




            # SERVIPORNO  CONTADOR PAGINAS

           # SALYNPM,  EstrenosCineSaa EROTICAGE PORNDISH NETFAPX xtits free18 hogtv sleazemovies esta en BS    
           # SPANKWIRE TXXX HDZOG es jsontools
           
           https://www.xozilla.com/player/kt_player.js?v=5.0.1   javtasty javwhores xozilla pornrewind camwhoresbay porntrex tubedupe
           
                                           def play(item):
                                                logger.info()
                                                itemlist = []
                                                data = httptools.downloadpage(item.url).data
                                                if "video_url_text" in data:  #si tiene definida calidad
                                                    patron = '(?:video_url|video_alt_url[0-9]*):\s*\'([^\']+)\'.*?'
                                                    patron += '(?:video_url_text|video_alt_url[0-9]*_text):\s*\'([^\']+)\''
                                                else:                         #si NO tiene definida calidad 1 link
                                                    patron = '(?:video_url|video_alt_url[0-9]*):\s*\'([^\']+)\'.*?'
                                                    patron += 'postfix:\s*\'([^\']+)\''
                                                matches = scrapertools.find_multiple_matches(data, patron)
                                                for url,quality in matches:
                                                    itemlist.append(['%s' %quality, url])
                                                return itemlist
           
#####################################################################################################


    data = httptools.downloadpage(item.url, forced_proxy_opt='ProxyCF', canonical=canonical)
    if data.sucess or data.code == 302:
        data = data.data


                         ##################################################################################################                              


url = httptools.downloadpage(url, headers=headers , follow_redirects=False, only_headers=True).headers.get("location", "")


            # PORNOXO  coger SCRIPT con soup y json
                    def prueba(item):
                        from core import jsontools as json
                        logger.info()
                        itemlist = []
                        url = "https://www.reuters.com/video/marketsnow"
                        data = httptools.downloadpage(item.url).data
                        soup = BeautifulSoup(data, "html5lib", from_encoding="utf-8")
                        soup = soup.find('script', type='application/ld+json').string
                        JSONData = json.load(soup)
                        url = JSONData['contentUrl']
                        logger.debug(url)
                        return                    
                        # <script type="application/ld+json" class="next-head">
                            # {
                                # "@context": "https://schema.org",
                                # "@type": "VideoObject",
                                # "name": "Bitcoin dive could signal drop for stocks: analyst | Reuters Video",
                                # "description": "Thomson Reuters Stocks Buzz analyst Terence Gabriel tells Reuters Fred Katayama he sees a correlation in the movements between bitcoin and U.S. equities.",
                                # "thumbnailUrl": "https://static.reuters.com/resources/r/?d=20210121&i=OVDW5KWWB&r=OVDW5KWWB&t=2",
                                # "uploadDate": "2021-01-21T20:54:30Z",
                                # "duration": "325",
                                # "contentUrl": "https://ajo.prod.reuters.tv/rest/v2/playlist/assets/432509/web.m3u8",
                                # "embedUrl": "https://reut.rs/3qTxfpN"
                            # }
                         ##################################################################################################                              
           
           
        # tnaflix pelisxporno headers referer
            headers = {'Referer': item.url}
            data = httptools.downloadpage(url, headers=headers).data
        # shameless  url headers referer diferente para sacar los links
        # LIKUOO  EROTICAGE  HDZOG HDCLIP  POST DOWNLOADPAGE GOTPORN con SOUP
          datas = httptools.downloadpage(scrapedurl, post=post, headers={'Referer':item.url}).data
                EROTICAGE   cine-matik.com

        #######  thepornfull  yespornplease   babestube fuqer
        import xbmc
        import xbmcgui

                                def play(item):
                                    logger.info()
                                    itemlist = []
                                    soup = create_soup(item.url)
                                    url = soup.find('div', class_='responsive-player').iframe['src']
                                    data = httptools.downloadpage(url).data
                                    url = scrapertools.find_single_match(data, 'file: "([^"]+)"')
                                    url += "|Referer=%s" % item.url
                                    listitem = xbmcgui.ListItem(item.title)
                                    listitem.setArt({'thumb': item.thumbnail, 'icon': "DefaultVideo.png", 'poster': item.thumbnail})
                                    listitem.setInfo('video', {'Title': item.title, 'Genre': 'Porn', 'plot': '', 'plotoutline': ''})
                                    listitem.setMimeType('application/vnd.apple.mpegurl')
                                    listitem.setContentLookup(False)
                                    # itemlist.append(Item(channel=item.channel, title= "%s", contentTitle = item.title, url=url ))
                                    # itemlist = servertools.get_servers_itemlist(itemlist, lambda i: i.title % i.server.capitalize())
                                    return xbmc.Player().play(url, listitem)

# ERROR: CCurlFile::Stat - Failed: Peer certificate cannot be authenticated with given CA certificates(60)
    url += "|verifypeer=false"

    url += "|ignore_response_code=True"     No hacer test_video_exists en server directo

    url += "|Referer=%s" % host

        # REDIRECCION de URL
            # https://strdef.world/vplayer.php?id=351ec419-8f56-4f93-8f5e-8e945c6ad399 = verystream 
            # https://strdef.world/player.php?id=609f6664-e500-4377-96cd-9737b0a3d21c = oload
            
                url = httptools.downloadpage(url).url # da la url a la que redireciona, verystream y openload 
 

 # bit.ly  SERVER STREAMZ redirecionar dll
         url = httptools.downloadpage(url, follow_redirects=False).headers["location"]


     #SERVIDOR MANGOVIDEO   https://pandamovies.pw     https://watchfreexxx.net/ Playpornx   xxxfreeinhd
 
    KTPLAYER : javtasty javwore mangovideo pornrewind 
 
        # SERVIDOR NUVID
         "pattern": "(http://www.(?:nuvid|drtuber|iceporn|viptube|vivatube|tubeon|hd21|yeptube|winporn).com/embed/[0-9]+)",
        # SERVIDOR TXX  txx y hclips hdzog 
        # SERVIDOR tubepornclassic, upornia pC3:'1600271572|2544074943,1715703815',
        # SEREVIDOR bravoporn bravoyube. xcafe, anyporn, alphaporno, xbabe, xcum, sex3 tubewolf, anysex

 
    # En play server directo y titulo
    itemlist.append( Item(channel=item.channel, action="play", title= item.title, server= "directo", url=url))


    # Findvideos calidades convertir en play   beeg shameless TNAFLIX xhamster yespornplease
            itemlist.append(["%s %s [directo]" % (quality, url), url])


    # https://www.pornhub.com/embed/ph5c08d470cfa36?ref=10037234
    url = scrapertools.find_single_match(data, '<div class="embed-wrap".*?<iframe src="([^"]+)\?ref=')


            ################    PORNLIB
        headers = {"Cookie": "cattype=%s; index_filter_sort=%s" % (cattype, ctype)}



    from channels import pornhub
    if "pornhub" in url : 
        item1 = item.clone(url=url)
        itemlist = pornhub.play(item1)
        return itemlist



################ ENCONTRAR LINEA QUE TIENE SRC en SOUP     ViralxVideos

def find_src(tag):
    return tag.has_attr('src')

def play(item):
    logger.info()
    itemlist = []
    soup = create_soup(item.url).find('div', class_='video-player')
    url = soup.find(find_src)
    # url = soup.find(src=re.compile(r"^http[A-z0-9=\/]+"))
    if url:
        url = url['src']

###############   PORNCOM    saltar link publicidad contiene atributo  'data-adch'
        if elem.has_attr('data-adch'):
            continue


                    matches = soup.find_all(attrs={"data-vid": re.compile(r"^\d+")})
 
 
 
            ####################################
            if title.get("sf_name", ""):   # comprueba que titulo tiene sf_name lo de abajo da error
            
                    if title["sf_name"]:
                        title = title["sf_name"]  
            
##############################################

####### Coge los 4 ultimos caracteres (.ext) y si es m3u8 lo descarta 

        extension = scrapertools.get_filename_from_url(videourl)[-4:]


    for videourl in matches:
        extension = videourl[-4:]
        if extension == 'm3u8':
            continue
        video_urls.append(["%s [vidlox]" % extension, videourl])


        [-4:] #COGE LOS 4 ULTIMOS CARACTERES DE LA CADENA
        [:-2] #QUITA LOS DOS ULTIMOS CARACTERES DE LA CADENA
        [1 : -1] # DESDE EL SEGUNDO CARACTER HASTA EL PENULTIMO
        
        
        
        itemlist[::-1] # invierte lista
        
        itemlist.reverse()                      #INVERTIR EL ORDEN DE LA LISTA
        itemlist.sort(key=lambda x: x.title)    #ORDENAR ALFABETICAMENTE POR TITULO
        return sorted(itemlist, key=lambda i: i.title)
        
        itemlist.sort(key=lambda item: int( re.sub("\D", "", item[0])))  #ordena por quality

        
        
        url = urlparse.unquote(url)  #QUITA %21 %3C DE LA URL Y PONE ://

        # PORNVIBE capitalice title con mezcla de la url
    if "lat" in lang.lower(): lang= "Lat"   #pone lang en minusculas y si esta lat el lang es Lat
    quality = quality.strip().upper()    .lower()   #quitar espacios y mayusculas o minusculas 
                            .capitalize()          # poner en mayuscula primera letra
    cantidad = cantidad.strip()             #quita los espacios antes y despues        
    title = " ".join(title.split())         #Quita los espacios entre palabras de title


    for url, lang in zip(video_urls, idioma):  Coge elemento de la lista video_urls y de la lista idioma


    logger.debug("ITEM: %s" % item)            logger.info("Intel11 %s" %item)
               scrapertools.printMatches(matches)



    id=re.compile(r"^list_videos_[A-z_]+")

    itemlist.append(item.clone(title="", action="", folder=False))  # Linea en blanco


#####    foxtube  play de cumlouder
                if "cumlouder" in url:
                    from channels import cumlouder
                    item1 = Item(channel=item.channel, url=url, contentTitle = item.contentTitle)
                    itemlist = cumlouder.play(item1)
                    return itemlist



    ([A-z0-9]+)
    ([^<]+)     #para extraer el texto entre dos tags “uno o más caracteres que no sean <" ^ cualquier caracter que no sea <
    \d+         #para saltar números
    \s+         #para saltar espacios en blanco
    (.*?)       #cuando la cosa se pone complicada
    ([^\s]+)    #todo lo que hay hasta espacio
    "([^"]+)"   #todo lo que hay hasta comillas
    \'([^\']+)\'
    ([^,"]+)    #todo lo que hay hasta , o "  TUBEXPORN
    
    
                        ######################    LISTAS POR CCOMPRESION    ###########################
                                                           
                            https://www.analyticslane.com/2019/09/23/listas-por-comprension-en-python/
                                                            
                lista = ["fembed", "myurlshort"]           # ====>                                                                              
                for x in lista:                            # ====>           if (x not in item.url for x in lista:                              
                    if x not in item.url:                  # ====>                                                                              
    
                ##############  INTERCAMBIO VALORES
                scrapedthumbnail, scrapedtitle = scrapedtitle, scrapedthumbnail

# DECODE ENCODE BASE64
        import base64
        url = base64.b64decode(url).decode('utf-8')
        url = base64.b64encode(url.encode('utf8')).decode('utf8')
        url = urlparse.unquote(url)  #QUITA %21 %3C DE LA URL Y PONE ://


########### ERROR bytes and str
            .decode("utf8")

    logger.debug(isinstance(url, bytes))

    
        if not url.startswith("https"):
            url = "https:%s" % url

        if not thumbnail.startswith("https"):
            thumbnail = "https:%s" % thumbnail

    scrapedtitle = scrapedtitle.replace("(%s)" %scrapedyear, "")  #QUITAR AÑO DEL TITULO

    # javtasty
    if item.extra == "play_menu":
        return itemlist, data
        
    video_urls, data = play(item.clone(extra="play_menu"))
    itemlist.append(item.clone(action="play", title="Ver -- %s" % item.title, video_urls=video_urls))
    #######

    img += "|Referer=https://www.porntrex.com/"     #REFERER
    url += "|Referer=%s" % host                     #referer tryboobs videos
    
    
    
                    ##### xxxfiles
        pornstars = elem.find('div', class_='thumb-models').find_all('a')
        for x , value in enumerate(pornstars):
            pornstars[x] = value.text.strip()
        pornstar = ' & '.join(pornstars)
        pornstar = "[COLOR cyan]%s[/COLOR]" % pornstar

    data = httptools.downloadpage(item.url, canonical=canonical).data
    patron = 'href="/models/[^"]+" title="([^"]+)"'
    pornstars = re.compile(patron,re.DOTALL).findall(data)



        ### fpo
    soup = create_soup(item.url).find('div', class_='info')
    matches = soup.find_all('div', class_='item')
    pornstars = matches[2].find_all('a')
    for x , value in enumerate(pornstars):
        pornstars[x] = value.text.strip()
    pornstar = ' & '.join(pornstars)
    pornstar = "[COLOR cyan]%s[/COLOR]" % pornstar
    lista = item.title.split()
    if "HD" in item.title:
        lista.insert (4, pornstar)
    else:
        lista.insert (2, pornstar)
    item.contentTitle = ' '.join(lista)


    #### joporn
    soup = create_soup(item.url)
    pornstars = soup.find_all('a', href=re.compile("/teg/"))
    for x , value in enumerate(pornstars):
        pornstars[x] = value.text.strip()
    pornstar = ' & '.join(pornstars)
    pornstar = "[COLOR cyan]%s[/COLOR]" % pornstar
    logger.debug(pornstar)
    lista = item.contentTitle.split()
    lista.insert (2, pornstar)
    item.contentTitle = ' '.join(lista)    


    (?:Videos|videos)<    #utiliza los dos textos en la busqueda
        new_url = scrapertools.find_single_match(data, '(?:IFRAME|iframe) src=(.*?) scrolling')
        patron = '"([0-9]+p)":"([^"]+)"'
        patron = '(?:video_url|video_alt_url[0-9]*):\s*\'([^\']+)\'.*?'
        patron += '(?:video_url_text|video_alt_url[0-9]*_text):\s*\'([^\']+)\''
        matches = scrapertools.find_multiple_matches(data, patron)
        # scrapertools.printMatches(matches)
        


    ####     Formato hora 1h 1m 1s  >>>>>>>  1:01:01    hqporner

        hora = scrapedtime.split()
        if len(hora) == 3 and len(hora[1]) < 3:
            hora[1] = "0%s" %hora[1]
        if len(hora[-1]) <3: hora[-1] = "0%s" %hora[-1]
        scrapedtime = ' '.join(hora)


        # pasar duracion en segundos a horas   BEEG
        
        segundos = Video["duration"]
        horas=int(segundos/3600)
        segundos-=horas*3600
        minutos=int(segundos/60)
        segundos-=minutos*60
        if segundos < 10:
            segundos = "0%s" %segundos
        if minutos < 10:
            minutos = "0%s" %minutos
        if horas == 00:
            duration = "%s:%s" % (minutos,segundos)
        else:
            duration = "%s:%s:%s" % (horas,minutos,segundos)

##########CUENTA PAGINAS     TUBEPORNCLASSIC   XOOZILLA
    last_page= scrapertools.find_single_match(data,'<a href=".*?/latest/(\d+)"><div style="display:inline">Last<')
    logger.debug(last_page)
    page = scrapertools.find_single_match(item.url, "(.*?)/\d+")
    current_page = scrapertools.find_single_match(item.url, ".*?/(\d+)")
    if last_page:
        last_page = int(last_page)
    if current_page:
        current_page = int(current_page)
    if current_page < last_page:
        current_page = current_page + 1
        next_page = "%s/%s" %(page,current_page)


###############              next_page = soup.find(attrs={"aria-label": "Next"})
#################             next_page = soup.find("a", string=re.compile(r"^Next"))
#############           next_page = soup.find('a', class_='pagination__link', string='Next')


    page = int(scrapertools.find_single_match(item.url, '&offset=([0-9]+)'))
    next_page = (page+ 48)
    if next_page:
        next_page = re.sub(r"&offset=\d+", "&offset={0}".format(next_page), item.url)
        itemlist.append(item.clone(action="lista", title="[COLOR blue]Página Siguiente >>[/COLOR]", url=next_page))


        ######## javwhores javtasty mangovideo
    next_page = scrapertools.find_single_match(data, '<li class="next"><a href="([^"]+)"')
    if "#" in next_page:
        next_page = scrapertools.find_single_match(data, '<li class="next">.*?data-parameters="([^"]+)">Next')
        next_page = next_page.replace(":", "=").replace(";", "&").replace("+from_albums", "")
        next_page = "?%s" % next_page
    if next_page:
        next_page = urlparse.urljoin(item.url,next_page)
        itemlist.append(item.clone(action="lista", title="[COLOR blue]Página Siguiente >>[/COLOR]", url=next_page) )
        ########### xtit
    next_page = scrapertools.find_single_match(data, '<li class="item-pagin is_last">.*?data-parameters="([^"]+)"')
    if next_page:
        next_page = next_page.replace(":", "=").split(";")
        next_page = "?%s&%s" % (next_page[0], next_page[1])
        next_page = urlparse.urljoin(item.url,next_page)
        itemlist.append( Item(channel=item.channel, action="lista", title=next_page, text_color="blue", 
                              url=next_page) )

            
        hosta = 'https://www.amateur.tv/v3/readmodel/cache/cams/%s/0/50/es'
        next_page = re.sub(r"\d+/50/", "{0}/50/".format(current_page), item.url)


# <li class="next"><a href="" data-action="ajax" data-container-id="list_videos_most_recent_videos_pagination" data-block-id="list_videos_most_recent_videos" data-parameters="sort_by:ctr;from:2">Adelante</a></li>
    next_page = soup.find('li', class_='next')
    if next_page:
        next_page = next_page.a['data-parameters'].split(":")[-1]
        if "from_videos" in item.url:
            next_page = re.sub(r"&from_videos=\d+", "&from_videos={0}".format(next_page), item.url)
        else:
            next_page = re.sub(r"&from=\d+", "&from={0}".format(next_page), item.url)



    next_page = soup.find('li', class_='page-current')
    if next_page and next_page.find_next_sibling("li"):
        next_page = next_page.find_next_sibling("li").a['data-parameters'].split(":")[-1]
        if "from_videos" in item.url:
            next_page = re.sub(r"&from_videos=\d+", "&from_videos={0}".format(next_page), item.url)
        else:
            next_page = re.sub(r"&from=\d+", "&from={0}".format(next_page), item.url)



# <li><a class="current">1</a></li><li><a href="https://www.erogarga.com/page/2/" class="inactive">2</a></li>
    next_page = soup.find('a', class_='current')
    if next_page and next_page.parent.find_next_sibling("li"):
        next_page = next_page.parent.find_next_sibling("li").a['href']


    pagination = soup.find('div', class_='navigation')
    next_page = ""
    if pagination:
        if pagination.span.find_next_sibling("a"):
            next_page = pagination.span.find_next_sibling("a")['href']
        if "#" in next_page:
            prev_page = scrapertools.find_single_match(item.url, "(.*?&search_start=)")
            page = pagination.span.find_next_sibling("a").text
            next_page = "%s%s" % (prev_page, page)
    if next_page:
        itemlist.append(item.clone(action="lista", title="[COLOR blue]Página Siguiente >>[/COLOR]", url=next_page) )



    next_page = soup.find('div', class_='pagMovidy')
    for elem in next_page:
        if "siguiente" in elem.text:
            next_page = elem['href']
            itemlist.append(item.clone(action="sagas", title="[COLOR blue]Página Siguiente >>[/COLOR]", url=next_page) )


        # Esto es para verificar si estan activos lo titulos inteligentes, si no estan activos agrega la calidad
        #  y el idioma, si lo estan se agregaran solo si define quality y language en el item
        if not config.get_setting('unify'):
            title =  '[COLOR red] %s [/COLOR] (%s)' % (calidad , idioma)
        else:
            title = ''
        itemlist.append(item.clone(action="play", title='%s'+title, url=url, language=idioma, quality=calidad ))


########    BUCLE en xxxfreinhd

    n = 2
    while n > 0:
        b64_url = scrapertools.find_single_match(data, '\(dhYas638H\("([^"]+)"\)')
        b64_url = base64.b64decode(b64_url + "=")
        b64_url = base64.b64decode(b64_url + "==")
        data = b64_url
        n -= 1

        if "tk/goto/" in url:
            n = 3
            while n > 0:
                url= url.replace("https://vshares.tk/goto/", "").replace("https://waaws.tk/goto/", "").replace("https://openloads.tk/goto/", "")
                url = base64.b64decode(url)
                n -= 1

    rep = True
    while rep == True:
        b64_data = scrapertools.find_single_match(data, '\(dhYas638H\("([^"]+)"\)')
        if b64_data:
            b64_url = base64.b64decode(b64_data + "=")
            b64_url = base64.b64decode(b64_url + "==")
            data = b64_url
        else:
            rep = False


         #Esto coge una direccion url referer y saca el head como me dijo intel gmobi. pero no iba
         url = url.replace("&amp;", "&")
         response = httptools.downloadpage(url, follow_redirects=False, add_referer=True)
         if response.data:
             url = scrapertools.find_single_match(response.data, 'src="([^"]+)"')
         else:
             url = response.headers.get("location", "")
        
        
    next_page = int(current_page) + 1 #incrementa en 1 despues de convertir la cadena de texto en numero

    
    data = httptools.downloadpage(item.url).data
    data = scrapertools.find_single_match(data,'<div class="sbi-header">Películas por género</div>(.*?)</ul>')
    data = re.sub(r"\n|\r|\t|&nbsp;|<br>", "", data)

    thumbnail = urlparse.urljoin(item.url,scrapedthumbnail)
    

    patron  = '<li class="border-radius-5 box-shadow">(.*?)</li>'
    matches = re.compile(patron,re.DOTALL).findall(data)
    # scrapertools.printMatches(matches)
    for match in matches:
        url = scrapertools.find_single_match(match,'<a href="([^"]+)"')
        title = scrapertools.find_single_match(match,'title="([^"]+)"')
        thumbnail = scrapertools.find_single_match(match,'<img src="([^"]+)"')
        duracion = scrapertools.find_single_match(match,'<div class="time-infos">([^"]+)<span class="time-img">')
        idioma = scrapertools.find_multiple_matches(match,'<img src="[^"]+" title="([^"]+)"')
        plot = scrapertools.find_single_match(match,'<p><strong>Sinopsis:</strong> (.*?)</p>')
        calidad = calidad.replace("Ahora en ", "")
        genero = scrapertools.find_single_match(match,'<strong>Genero</strong>:\s+([^"]+)</div>')
        idioma = scrapertools.find_single_match(match,'<strong>Idioma</strong>:([^"]+)</div>')
        year = scrapertools.find_single_match(match,'</strong>:\s+(\d+)</div>')
        calidad = scrapertools.find_single_match(match,'<strong>Calidad</strong>:(.*?)</div>')
        thumbnail = host + thumbnail
        title = title.replace("Ver Película","").replace("ver película","").replace("ver pelicula","").replace("Online Gratis","")
        title = scrapertools.htmlclean(title).strip()
        plot = ""




    id = scrapertools.find_single_match(data, '"videoID":(\d+),')
    rd = int(id)/1000*1000
    if rd == 0:
        rd = "00000"
    url = "https://pornercdns.com/hls/00%s/%s/master.m3u8" %(rd,id)     



#DESCIFRADO QUE REALICE EN UN CANAL QUE YA HA CAMBIADO
		#                                   a b c d e f g h i j k l m n o p q r s t u v w y x z $ = & ? ( ^
#                                   h     e   o f   d     n a m   u t s       p   l r i : / . ? = &


    rep1 = {'a':'h', 'm':'a', 'n':'m', 'd':'e', 'f':'o', 'i':'d', 'p':'u', 'q':'t',
            'r':'s','v':'p', 'x':'r', 'z':'i', '$':':', '&':'.', '(':'=', '^':'&'}
    rep2 = {'g':'f', 'l':'n'}
    rep3 = {'y':'l'}
def replace_all(text, dic):
    for i, j in dic.iteritems():
        text = text.replace(i, j)
    return text



                        # TXX  UNICODE
    # txt = txt.replace('\u0410', 'A').replace('\u0412', 'B').replace('\u0421', 'C').replace('\u0415', 'E').replace('\u041c', 'M').replace('~', '=').replace(',','/')
    txt = txt.decode('unicode-escape').encode('utf8')
    txt = txt.replace('А', 'A').replace('В', 'B').replace('С', 'C').replace('Е', 'E').replace('М', 'M').replace('~', '=').replace(',','/')




dic = {'A':'A',      #\u0410
       'B':'B',      #\u0411
       'C':'C',      #\u0412
       'E':'E',      #\u0415
       'M':'M',      #\u041c
       '~':'=',
       ',':'/'
       }

    txt = 'AAMMCCDDEE'
    for i, j in dic.iteritems():
        text = txt.replace(i, j)

          ###########   PORNHUB SERVER reordenar videourl
    videourl = scrapertools.find_multiple_matches(data, 'var media_\d+=([^;]+)')
    for elem in videourl:
        orden = scrapertools.find_multiple_matches(elem, '\*\/([A-z0-9]+)')
        url= ""
        for i in orden:
            url += scrapertools.find_single_match(data, '%s="([^"]+)"' %i)
        if not "/get_media?" in url and not "urlset" in url:
            quality = scrapertools.find_single_match(url, '(\d+)P_')
            video_urls.append(["%sp [pornhub]" % quality, url])


def play(item):
    logger.info()
    itemlist = []
    data = httptools.downloadpage(item.url).data
    patron  = '<source src="([^"]+)" type="video/mp4" label="([^"]+)"'
    matches = scrapertools.find_multiple_matches(data, patron)
    for scrapedurl,scrapedtitle  in matches:
        itemlist.append(item.clone(action="play", title=scrapedtitle, contentTitle = item.title, url=scrapedurl))
    return itemlist


def play(item):
    logger.info(item)
    itemlist = []
    itemlist = servertools.find_video_items(item.clone(url = item.url, contentTitle = item.title))
    return itemlist


def play(item):
    logger.info()
    itemlist = []
    data = httptools.downloadpage(item.url).data
    url  = scrapertools.find_single_match(data,'<iframe src="([^"]+)"')
    itemlist.append(item.clone(action="play", title= "%s", contentTitle = item.title, url=url))
    itemlist = servertools.get_servers_itemlist(itemlist, lambda i: i.title % i.server.capitalize())
    return itemlist
    
##################################            
# spankbang, camwhore, pontrex
##################################
def play(item):
    logger.info()
    itemlist = []
    data = httptools.downloadpage(item.url).data
    patron = '(?:video_url|video_alt_url[0-9]*):\s*\'([^\']+)\'.*?'
    patron += '(?:video_url_text|video_alt_url[0-9]*_text):\s*\'([^\']+)\''
    matches = re.compile(patron,re.DOTALL).findall(data)
    for url,quality in matches:
        itemlist.append(['.mp4 %s' %quality, url])
    return itemlist


###########   CALIDAD M3U       ########
    # https://www.asspoint.com   https://www.ghettotube.com   https://www.porntitan.com https://www.porntv.com https://www.teenieporn.com 
    # https://www.asianpornmovies.com  https://www.cartoonpornvideos.com  https://www.lesbianpornvideos.com 
    # https://www.sexoasis.com https://www.youngpornvideos.com


def play(item):
    logger.info()
    itemlist = []
    data = httptools.downloadpage(item.url).data
    data = re.sub(r"\n|\r|\t|&nbsp;|<br>|<br/>", "", data)
    url = scrapertools.find_single_match(data, 'file: "([^"]+)"')
    url= urlparse.urljoin(host,url)
    m3u_data = httptools.downloadpage(url).data
    matches = scrapertools.find_multiple_matches(m3u_data, 'TION=\d+x(\d+).*?\s(.*?)\s')
    filename = scrapertools.get_filename_from_url(url)[-4:]
    if matches:
        for quality, url in matches:
            itemlist.append(["%s  %sp [bitporno]" % (filename, quality), url])
    return itemlist




def play(item):
    logger.info()
    itemlist = []
    data = httptools.downloadpage(item.url).data
    m3u = scrapertools.find_single_match(data, 'file: "([^"]+)"')
    data = httptools.downloadpage(m3u).data
    patron = 'RESOLUTION=\d+x(\d+),.*?'
    patron += '(index-.*?).m3u8'
    matches = re.compile(patron,re.DOTALL).findall(data)
    for quality,url in matches:
        url = m3u.replace("master", url)
        itemlist.append(['%sp' %quality, url])
    itemlist.sort(key=lambda item: int( re.sub("\D", "", item[0])))
        # itemlist.append(['.mp4 %s' %quality, url])
    # if len(itemlist) == 2:
        # itemlist.reverse()
    return itemlist

         ########   PORNHUB      ######
         
    # Poner el primer elemento "1080p" el ultimo
    if "1080p" in video_urls[0][0]:
        video_urls += video_urls[:1]
        video_urls.pop(0)
                    #### Beeg reordenar
                quality = videourl.replace("p", "" )
                itemlist.append(["%sp %s [directo]" % (quality, url[-4:]), url, quality])
    itemlist.sort(key=lambda item: int(item[2]))
    
    
    
        # Spankbang
    for quality,url in matches:
        if "4k" in quality:
            quality = "2160p"
        video_urls.append(['%s [.mp4]' %quality, url])
    video_urls.sort(key=lambda item: int( re.sub("\D", "", item[0])))





# autoplay comeolitas si findvideo definido   y aprobechado en pornrewind 
                # while not platformtools.is_playing():
                    # for ítem in itemlist:
                        # Launcher.run(ítem)
                        # Sleep(2)
                    # if platformtools.is_playing():
                        # Break



def play(item):
    itemlist = []
    itemlist = servertools.find_video_items(item) #findvideo en la url que se le pasa
    a = len (itemlist)
    for i in itemlist:
        if a < 1:
            return []
        res = servertools.check_video_link(i.url, i.server, timeout=5)
        a -= 1
        if 'green' in res:
            return [i]
        else:
            continue

def play(item):
    logger.info()
    itemlist = []
    data = httptools.downloadpage(item.url).data
    patron = '<iframe src="([^"]+)"'
    matches = re.compile(patron,re.DOTALL).findall(data)
    for scrapedurl in matches:
        itemlist.append(item.clone(action="play", title= "%s", contentTitle= item.title, url=scrapedurl))
    itemlist = servertools.get_servers_itemlist(itemlist, lambda i: i.title % i.server.capitalize())
    a = len (itemlist)
    for i in itemlist:
        
        if a < 1:
            return []
        res = servertools.check_video_link(i.url, i.server, timeout=5)
        a -= 1
        if 'green' in res:
            return [i]
        else:
            continue
            
            
            #####            
    a = len (itemlist)
    for i in itemlist:
        if a < 1:
            return []
        if 'clipwatching' in i.url:
            res = ""
        elif 'mangovideo' in i.url:
            res = ""
        else:
            res = servertools.check_video_link(i.url, i.server, timeout=5)
        a -= 1
        if 'green' in res:
            return [i]
        else:
            continue
            

